# -*- coding: utf-8 -*-
"""Heart_Disease_Dataset_220422.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v63UUO2wM3gzGgrgTgFEMV6FhoZvKM4E
"""

# Commented out IPython magic to ensure Python compatibility.
#importing necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
#sns.set(color_codes = True)
import matplotlib.pyplot as plt
# %matplotlib inline
import math
from IPython.display import display
from sklearn.utils import resample
from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV
from imblearn.datasets import make_imbalance
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_samples, silhouette_score
from scipy.stats import zscore
import warnings
warnings.filterwarnings("ignore")
print('Numpy version', np.__version__)
print('Pandas version', pd.__version__)
print('Seaborn version',sns.__version__)

# Heart Failure Prediction dataset
df_heart = pd.read_csv('heart.csv')

# to display all columns in the dataset
pd.options.display.max_columns = None

# Function to detect missing values and duplicate records

def missing_duplicates(col):
    if col.isnull().sum().sum() == 0:
        print('There are no missing values in the dataset')
    else:
        print('There are missing values in the dataset')
        
    count = 0
    for count in col.columns:
        if col[count].isnull().sum() != 0:
            print('There are {} missing values in the feature: '.format(col[count].isnull().sum()),count)
            print('Percentage of missing values in the feature:',round(col[count].isnull().sum()/col.shape[0] * 100,3),'%')
            print('Percentage of missing values in total data points:',round(col[count].isnull().sum()/(col.shape[0]*col.shape[1]) * 100,3),'%')
            
    if col.duplicated().sum() == 0:
        print('There are no duplicate records found in the dataset')
    else:
        print('There are duplicate rows found in the dataset.','\nTotal duplicate rows:', col.duplicated.sum())

# Function to check balance of the target variable and countplot
def balance_ratio(col):
    print('Balance ratio of variable in Percentage')
    print(round(col.value_counts(normalize=True) * 100,2))
    print('')
    sns.countplot(col)
    plt.title('Count plot')
    plt.show()

# Fuctions to plot the graphs for countplot, histograms and boxplot

# Function to plot the graph of countplots
def countplots(col,figsize_len,figsize_wid,column):
    col = col.select_dtypes(include=['object'])
    plt.figure(figsize=(figsize_len,figsize_wid))
    a,b,c = math.ceil(col.shape[1]/column),column,1
    i = 0
    for i in col.columns:
        if col[i].dtype == 'object':
            plt.subplot(a,b,c)
            sns.countplot(x=col[i])
            c = c + 1
    plt.show()
    return

# Function to plot the graph of Histograms
def histogram(col,figsize_len,figsize_wid,column):
    col = col.select_dtypes(exclude='object')
    plt.figure(figsize=(figsize_len,figsize_wid))
    a,b,c = math.ceil(col.shape[1]/column),column,1
    i = 0
    for i in col.columns:
        if col[i].dtype != 'object':
            plt.subplot(a,b,c)
            sns.distplot(col[i])
            c = c + 1
    plt.show()
    return

# Function to plot the graph of boxplots
def boxplots(col,figsize_len,figsize_wid,column):
    col = col.select_dtypes(exclude='object')
    plt.figure(figsize=(figsize_len,figsize_wid))
    a,b,c = math.ceil(col.shape[1]/column),column,1
    i = 0
    for i in col.columns:
        if col[i].dtype != 'object':
            plt.subplot(a,b,c)
            sns.boxplot(col[i])
            c = c + 1
    plt.show()
    return

# Function to create data imbalance
def create_imbalance(df,target_index,Perc):
    data = df.copy()
    
    # seperating independent and dependent variables
    X = data.drop(data.columns[target_index],axis=1)
    y = data.iloc[:,target_index]
    
    # creating imbalance from given percentage
    ratio1 = int(y.value_counts().sort_values(ascending=False)[0] * (Perc/100))
    ratio2 = int(y.value_counts().sort_values(ascending=False)[0]) - ratio1
    label1 = data.iloc[:,target_index].value_counts().sort_values(ascending=False).index[0]
    label2 = data.iloc[:,target_index].value_counts().sort_values(ascending=False).index[1]
    
    #making imbalance
    X_res, y_res = make_imbalance(X, y, sampling_strategy={label1:ratio1, label2:ratio2}, random_state=1)
    target_variable = data.columns[target_index]
    X_res[target_variable] = y_res
    
    return X_res

"""# Heart Failure Prediction dataset"""

# now lets exlore the heart failure dataset

# checking the top 5 rows of the data
df_heart.head()

# information of the data
df_heart.info()

# describing the data
df_heart.describe().T

# Checking missing values and duplicates
missing_duplicates(df_heart)

df_heart['HeartDisease'] = np.where(df_heart['HeartDisease'] == 0,'No','Yes')

# Checking the target feature balance
balance_ratio(df_heart['HeartDisease'])

# we will check countplots for categorical variables
countplots(df_heart,15,12,3)

histogram(df_heart,15,8,3)

boxplots(df_heart,15,8,3)

#Bivariate Analysis using heatmap
plt.figure(figsize=(15,10))
sns.heatmap(round(df_heart.corr(),2),annot=True,cmap='Blues')
plt.show()

# pairplot
sns.pairplot(df_heart, corner=True)

df_heart.columns

# We will perform one hot encoding on all categorical variables.
# Drop_first is set to True in order to avoid multicolinearity and also it reduces number of features
# We will not encode the target variable i.e 'HeartDisease'
df_heart = pd.get_dummies(data=df_heart, columns=['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'], drop_first=True)
df_heart.head()

df_heart.shape

# creating 65:35 imbalance ratio
df_heart_65 = create_imbalance(df_heart,6,65)
balance_ratio(df_heart_65['HeartDisease'])

# creating 75:25 imbalance ratio
df_heart_75 = create_imbalance(df_heart,6,75)
balance_ratio(df_heart_75['HeartDisease'])

# creating 90:10 imbalance ratio
df_heart_90 = create_imbalance(df_heart,6,90)
balance_ratio(df_heart_90['HeartDisease'])

"""**Base Line Model**"""

# grid search to find best hyper tuning parameters

# configuring min max sacling
scale_minmax = MinMaxScaler()

# seperating target and features
X = df_heart.drop(columns=['HeartDisease'],axis=1)
y = df_heart['HeartDisease']

X = pd.DataFrame(scale_minmax.fit_transform(X), columns=X.columns)


grid = {
    'max_features': [4,5],
    'max_depth': [4,5,6],
    'min_samples_leaf': [5,10], 
    'min_samples_split': [2,4,6],
       }

RF_Model = RandomForestClassifier(random_state=1)

grid_search = GridSearchCV(estimator = RF_Model, param_grid = grid, cv = 10,n_jobs=-1,scoring='f1')
grid_search.fit(X, y)
print(grid_search.best_params_,'\n')

# configuring stratified cross validation with cv=10
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)

# configuring scaling
scale = MinMaxScaler()

# for original data

# seperating target and features
X = df_heart.drop(columns=['HeartDisease'],axis=1)
y = df_heart['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='No')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='No')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_original = train_acc
test_acc_BL_original = test_acc
train_f1_BL_original = train_f1
test_f1_BL_original = test_f1
train_kappa_BL_original = train_kappa
test_kappa_BL_original = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

# for low imbalance data

# seperating target and features
X = df_heart_65.drop(columns=['HeartDisease'],axis=1)
y = df_heart_65['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='No')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='No')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_low = train_acc
test_acc_BL_low = test_acc
train_f1_BL_low = train_f1
test_f1_BL_low = test_f1
train_kappa_BL_low = train_kappa
test_kappa_BL_low = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

# for med imbalance data

# seperating target and features
X = df_heart_75.drop(columns=['HeartDisease'],axis=1)
y = df_heart_75['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='No')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='No')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_med = train_acc
test_acc_BL_med = test_acc
train_f1_BL_med = train_f1
test_f1_BL_med = test_f1
train_kappa_BL_med = train_kappa
test_kappa_BL_med = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

# for high imbalance data

# seperating target and features
X = df_heart_90.drop(columns=['HeartDisease'],axis=1)
y = df_heart_90['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='No')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='No')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_hig = train_acc
test_acc_BL_hig = test_acc
train_f1_BL_hig = train_f1
test_f1_BL_hig = test_f1
train_kappa_BL_hig = train_kappa
test_kappa_BL_hig = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

"""**K-means clustering for original data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable, 1 for satisfied and 0 for other
df_heart['HeartDisease'] = np.where(df_heart['HeartDisease'] == 'No', 0, df_heart['HeartDisease'])
df_heart['HeartDisease'] = np.where(df_heart['HeartDisease'] == 'Yes', 1, df_heart['HeartDisease'])

# checking optimal value of k using elbow method
df_heart_scale = pd.DataFrame(scale_minmax.fit_transform(df_heart), columns=df_heart.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(1,8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_heart_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_heart_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_heart_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_heart['HeartDisease'] = np.where(df_heart['HeartDisease'] == 0, 'No', df_heart['HeartDisease'])
df_heart['HeartDisease'] = np.where(df_heart['HeartDisease'] == 1, 'Yes', df_heart['HeartDisease'])

# for original data

# seperating target and features
X = df_heart.drop(columns=['HeartDisease'],axis=1)
y = df_heart['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=0)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_org = train_f1
test_f1_cluster0_org = test_f1

# for original data

# seperating target and features
X = df_heart.drop(columns=['HeartDisease'],axis=1)
y = df_heart['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_org = train_f1
test_f1_cluster1_org = test_f1

"""**K-means clustering for low imbalance data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable, 1 for satisfied and 0 for other
df_heart_65['HeartDisease'] = np.where(df_heart_65['HeartDisease'] == 'No', 0, df_heart_65['HeartDisease'])
df_heart_65['HeartDisease'] = np.where(df_heart_65['HeartDisease'] == 'Yes', 1, df_heart_65['HeartDisease'])

# checking optimal value of k using elbow method
df_heart_scale = pd.DataFrame(scale_minmax.fit_transform(df_heart_65), columns=df_heart_65.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(1,8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_heart_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_heart_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_heart_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_heart_65['HeartDisease'] = np.where(df_heart_65['HeartDisease'] == 0, 'No', df_heart_65['HeartDisease'])
df_heart_65['HeartDisease'] = np.where(df_heart_65['HeartDisease'] == 1, 'Yes', df_heart_65['HeartDisease'])

# for low imbalance data

# seperating target and features
X = df_heart_65.drop(columns=['HeartDisease'],axis=1)
y = df_heart_65['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart_65.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=0)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart_65.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_low = train_f1
test_f1_cluster0_low = test_f1

# for low imbalance data

# seperating target and features
X = df_heart_65.drop(columns=['HeartDisease'],axis=1)
y = df_heart_65['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart_65.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 1]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=0)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart_65.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_low = train_f1
test_f1_cluster1_low = test_f1

"""**K-means clustering for med imbalance data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable, 1 for satisfied and 0 for other
df_heart_75['HeartDisease'] = np.where(df_heart_75['HeartDisease'] == 'No', 0, df_heart_75['HeartDisease'])
df_heart_75['HeartDisease'] = np.where(df_heart_75['HeartDisease'] == 'Yes', 1, df_heart_75['HeartDisease'])

# checking optimal value of k using elbow method
df_heart_scale = pd.DataFrame(scale_minmax.fit_transform(df_heart_75), columns=df_heart_75.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(1,8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_heart_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_heart_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_heart_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_heart_75['HeartDisease'] = np.where(df_heart_75['HeartDisease'] == 0, 'No', df_heart_75['HeartDisease'])
df_heart_75['HeartDisease'] = np.where(df_heart_75['HeartDisease'] == 1, 'Yes', df_heart_75['HeartDisease'])

# for med imbalance data

# seperating target and features
X = df_heart_75.drop(columns=['HeartDisease'],axis=1)
y = df_heart_75['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart_75.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=0)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart_75.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_med = train_f1
test_f1_cluster0_med = test_f1

# for med imbalance data

# seperating target and features
X = df_heart_75.drop(columns=['HeartDisease'],axis=1)
y = df_heart_75['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart_75.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 1]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=0)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart_75.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_med = train_f1
test_f1_cluster1_med = test_f1

"""**K-means clustering for high imbalance data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable
df_heart_90['HeartDisease'] = np.where(df_heart_90['HeartDisease'] == 'No', 0, df_heart_90['HeartDisease'])
df_heart_90['HeartDisease'] = np.where(df_heart_90['HeartDisease'] == 'Yes', 1, df_heart_90['HeartDisease'])

# checking optimal value of k using elbow method
df_heart_scale = pd.DataFrame(scale_minmax.fit_transform(df_heart_90), columns=df_heart_90.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(1,8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_heart_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_heart_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_heart_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_heart_90['HeartDisease'] = np.where(df_heart_90['HeartDisease'] == 0, 'No', df_heart_90['HeartDisease'])
df_heart_90['HeartDisease'] = np.where(df_heart_90['HeartDisease'] == 1, 'Yes', df_heart_90['HeartDisease'])

# for high imbalance data

# seperating target and features
X = df_heart_90.drop(columns=['HeartDisease'],axis=1)
y = df_heart_90['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart_90.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=0)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart_90.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_hig = train_f1
test_f1_cluster0_hig = test_f1

# for high imbalance data

# seperating target and features
X = df_heart_90.drop(columns=['HeartDisease'],axis=1)
y = df_heart_90['HeartDisease']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['HeartDisease'] = y_test_fold 
  df_train['HeartDisease'] = y_train_fold
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'No', 0, df_train['HeartDisease'])
  df_train['HeartDisease'] = np.where(df_train['HeartDisease'] == 'Yes', 1, df_train['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'No', 0, test_data['HeartDisease'])
  test_data['HeartDisease'] = np.where(test_data['HeartDisease'] == 'Yes', 1, test_data['HeartDisease'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_heart_90.columns) 
  km_3 = KMeans(n_clusters=2,random_state=1)    # k=2 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_heart_cluster = df_train_scale.copy()
  df_heart_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_heart_cluster[df_heart_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.HeartDisease.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['HeartDisease'], axis=1)
    labels = df_cluster0['HeartDisease']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=4, max_depth=4, min_samples_leaf=5, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_heart_90.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['HeartDisease'], axis=1)
    y_test = test_data_scale['HeartDisease']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=0)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_hig = train_f1
test_f1_cluster1_hig = test_f1

"""**Comparison using permutation test**"""

from numpy.random.mtrand import permutation
def get_pvalue(iterations,Results1,Results2,diff):
  concat = np.concatenate((Results1,Results2))
  count = 0
  for i in range(0,iterations):
    permutation = np.random.permutation(concat)
    p_current = permutation[:len(Results1)]
    p_new = permutation[len(Results1):]
    mean_permutation_current = p_current.mean()
    mean_permutation_new = p_new.mean()
    t_permutation = mean_permutation_new - mean_permutation_current

    if(t_permutation > diff):
      count += 1
  p_value = count / iterations
  if p_value > 0.05:
    print('Since p value {} is greater than 0.05 we failed to reject null hypothesis'.format(round(p_value,4)))
    #print('Results 1 are not better than Results 2')
  else:
    print('Since p value {} is less than 0.05 we will reject null hypothesis'.format(round(p_value,4)))
    #print('Results 1 are better than Results 2')
  return #p_value

# comaprison of baseline model for different imbalances
comparison = pd.concat([pd.Series(test_f1_BL_original), pd.Series(test_f1_BL_low), pd.Series(test_f1_BL_med), pd.Series(test_f1_BL_hig)], axis=1)
comparison.columns = ['Original Data', 'Low Imbalance', 'Medium Imbalance', 'high Imbalance']
comparison.boxplot()
plt.ylabel('F1-score')
plt.xticks(rotation=45)
plt.show()

# comaprison of custom model for different imbalances
comparison1 = pd.concat([pd.Series(test_f1_cluster0_org), pd.Series(test_f1_cluster1_org), pd.Series(test_f1_cluster0_low), pd.Series(test_f1_cluster1_low), pd.Series(test_f1_cluster0_med), pd.Series(test_f1_cluster1_med), pd.Series(test_f1_cluster0_hig), pd.Series(test_f1_cluster1_hig)], axis=1)
comparison1.columns = ['Org. Cluster 0 ', 'Org. Cluster 1', 'Low. Cluster 0 ', 'Low. Cluster 1', 'Med. Cluster 0 ', 'Med. Cluster 1', 'Hig. Cluster 0 ', 'Hig. Cluster 1']
comparison1.boxplot()
plt.ylabel('F1-score')
plt.xticks(rotation=90)
plt.show()

comparison

comparison1

"""**Comparison between base and custom models**"""

# original data comparison base line Vs custo model
diff = test_f1_BL_original.mean() - test_f1_cluster0_org.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_original, test_f1_cluster0_org, diff)

diff = test_f1_BL_original.mean() - test_f1_cluster1_org.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_original, test_f1_cluster1_org, diff)

# low data imbalance comparison base line Vs custo model
diff = test_f1_BL_low.mean() - test_f1_cluster0_low.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_low, test_f1_cluster0_low, diff)

diff = test_f1_BL_low.mean() - test_f1_cluster1_low.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_low, test_f1_cluster1_low, diff)

# Medium data imbalance comparison base line Vs custo model
diff = test_f1_BL_med.mean() - test_f1_cluster0_med.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_med, test_f1_cluster0_med, diff)

diff = test_f1_BL_med.mean() - test_f1_cluster1_med.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_med, test_f1_cluster1_med, diff)

# High data imbalance comparison base line Vs custo model
diff = test_f1_BL_hig.mean() - test_f1_cluster0_hig.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_hig, test_f1_cluster0_hig, diff)

diff = test_f1_BL_hig.mean() - test_f1_cluster1_hig.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_hig, test_f1_cluster1_hig, diff)