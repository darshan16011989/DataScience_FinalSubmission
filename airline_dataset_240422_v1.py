# -*- coding: utf-8 -*-
"""Airline_Dataset_240422_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/162v1ggbK8GvYNCIAV5R0Xl_idOupfPaI
"""

# Commented out IPython magic to ensure Python compatibility.
#importing necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
#sns.set(color_codes = True)
import matplotlib.pyplot as plt
# %matplotlib inline
import math
from IPython.display import display
from sklearn.utils import resample
from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV
from imblearn.datasets import make_imbalance
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_samples, silhouette_score
from scipy.stats import zscore
import warnings
warnings.filterwarnings("ignore")
print('Numpy version', np.__version__)
print('Pandas version', pd.__version__)
print('Seaborn version',sns.__version__)

# Airline Passenger satisfication dataset
df_airline = pd.read_csv('Airline_Passenger_Data.csv')
df_airline.head()

# to display all columns in the dataset
pd.options.display.max_columns = None

# Function to detect missing values and duplicate records

def missing_duplicates(col):
    if col.isnull().sum().sum() == 0:
        print('There are no missing values in the dataset')
    else:
        print('There are missing values in the dataset')
        
    count = 0
    for count in col.columns:
        if col[count].isnull().sum() != 0:
            print('There are {} missing values in the feature: '.format(col[count].isnull().sum()),count)
            print('Percentage of missing values in the feature:',round(col[count].isnull().sum()/col.shape[0] * 100,3),'%')
            print('Percentage of missing values in total data points:',round(col[count].isnull().sum()/(col.shape[0]*col.shape[1]) * 100,3),'%')
            
    if col.duplicated().sum() == 0:
        print('There are no duplicate records found in the dataset')
    else:
        print('There are duplicate rows found in the dataset.','\nTotal duplicate rows:', col.duplicated.sum())

# Function to check balance of the target variable and countplot
def balance_ratio(col):
    print('Balance ratio of variable in Percentage')
    print(round(col.value_counts(normalize=True) * 100,2))
    print('')
    sns.countplot(col)
    plt.title('Count plot')
    plt.show()

# Fuctions to plot the graphs for countplot, histograms and boxplot

# Function to plot the graph of countplots
def countplots(col,figsize_len,figsize_wid,column):
    col = col.select_dtypes(include=['object'])
    plt.figure(figsize=(figsize_len,figsize_wid))
    a,b,c = math.ceil(col.shape[1]/column),column,1
    i = 0
    for i in col.columns:
        if col[i].dtype == 'object':
            plt.subplot(a,b,c)
            sns.countplot(x=col[i])
            c = c + 1
    plt.show()
    return

# Function to plot the graph of Histograms
def histogram(col,figsize_len,figsize_wid,column):
    col = col.select_dtypes(exclude='object')
    plt.figure(figsize=(figsize_len,figsize_wid))
    a,b,c = math.ceil(col.shape[1]/column),column,1
    i = 0
    for i in col.columns:
        if col[i].dtype != 'object':
            plt.subplot(a,b,c)
            sns.distplot(col[i])
            c = c + 1
    plt.show()
    return

# Function to plot the graph of boxplots
def boxplots(col,figsize_len,figsize_wid,column):
    col = col.select_dtypes(exclude='object')
    plt.figure(figsize=(figsize_len,figsize_wid))
    a,b,c = math.ceil(col.shape[1]/column),column,1
    i = 0
    for i in col.columns:
        if col[i].dtype != 'object':
            plt.subplot(a,b,c)
            sns.boxplot(col[i])
            c = c + 1
    plt.show()
    return

"""# Airline Passenger dataset"""

# information of the data
df_airline.info()

# describing the data
df_airline.describe().T

# we will be dropping column1 feature since they are unique values of rows
# we will be dropping customer feature since they are unique values

df_airline.drop(columns=['Column1','id'], axis=1, inplace=True)

# Checking the target feature balance
balance_ratio(df_airline['satisfaction'])

# we will check countplots for categorical variables
# countplots(df_airline,15,12,2)

#histogram(df_airline,15,18,4)

#boxplots(df_airline,18,15,5)

#Bivariate Analysis using heatmap
#plt.figure(figsize=(15,10))
#sns.heatmap(round(df_airline.corr(),2),annot=True,cmap='Blues')
#plt.show()

# pairplot
#sns.pairplot(df_airline, corner=True)

# Scatter plot with target variable
#plt.figure(figsize=(10,10))
#sns.scatterplot(x=df_airline['Departure Delay in Minutes'], y=df_airline['Arrival Delay in Minutes'],hue=df_airline['satisfaction'])
#plt.show()

# Checking for missing values and duplicate records
#missing_duplicates(df_airline)

# Since there are only 0.3% of missing values in "Arrival Delay in Minutes", we prefer to drop rows containing missing values
print('There are {} rows and {} columns before dropping null values:'.format(df_airline.shape[0],df_airline.shape[1]))
df_airline.dropna(axis=0, subset=['Arrival Delay in Minutes'], inplace=True)
print('There are {} rows and {} columns after dropping null values:'.format(df_airline.shape[0],df_airline.shape[1]))

# Checking for missing values and duplicate records after dropping null values
missing_duplicates(df_airline)

# pulling column names
df_airline.columns

# We will perform one hot encoding on all categorical variables.
# Drop_first is set to True in order to avoid multicolinearity and also it reduces number of features

df_airline = pd.get_dummies(data=df_airline, columns=['Gender','Customer Type','Type of Travel','Class'], drop_first=True)
df_airline.head()

df_airline.shape

# Function to create data imbalance
def create_imbalance(df,target_index,Perc):
    data = df.copy()
    
    # seperating independent and dependent variables
    X = data.drop(data.columns[target_index],axis=1)
    y = data.iloc[:,target_index]
    
    # creating imbalance from given percentage
    ratio1 = int(y.value_counts().sort_values(ascending=False)[0] * (Perc/100))
    ratio2 = int(y.value_counts().sort_values(ascending=False)[0]) - ratio1
    label1 = data.iloc[:,target_index].value_counts().sort_values(ascending=False).index[0]
    label2 = data.iloc[:,target_index].value_counts().sort_values(ascending=False).index[1]
    
    #making imbalance
    X_res, y_res = make_imbalance(X, y, sampling_strategy={label1:ratio1, label2:ratio2}, random_state=1)
    target_variable = data.columns[target_index]
    X_res[target_variable] = y_res
    
    return X_res

# creating 65:35 imbalance ratio
df_airline_65 = create_imbalance(df_airline,18,65)
balance_ratio(df_airline_65['satisfaction'])

# creating 75:25 imbalance ratio
df_airline_75 = create_imbalance(df_airline,18,75)
balance_ratio(df_airline_75['satisfaction'])

# creating 90:10 imbalance ratio
df_airline_90 = create_imbalance(df_airline,18,90)
balance_ratio(df_airline_90['satisfaction'])

"""**Base line Model**"""

df_airline_90.shape

# grid search to find best hyper tuning parameters

# configuring min max sacling
scale_minmax = MinMaxScaler()

# seperating target and features
X = df_airline.drop(columns=['satisfaction'],axis=1)
y = df_airline['satisfaction']

X = pd.DataFrame(scale_minmax.fit_transform(X), columns=X.columns)


grid = {
    'max_features': [5,6],
    'max_depth': [4,5,6],
    'min_samples_leaf': [1000,2000], 
    'min_samples_split': [2,4],
       }

RF_Model = RandomForestClassifier(random_state=1)

grid_search = GridSearchCV(estimator = RF_Model, param_grid = grid, cv = 10,n_jobs=-1,scoring='f1')
grid_search.fit(X, y)
print(grid_search.best_params_,'\n')

# configuring stratified cross validation with cv=10
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)

# configuring scaling
scale = StandardScaler()

# for original data

# seperating target and features
X = df_airline.drop(columns=['satisfaction'],axis=1)
y = df_airline['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='satisfied')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='satisfied')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_original = train_acc
test_acc_BL_original = test_acc
train_f1_BL_original = train_f1
test_f1_BL_original = test_f1
train_kappa_BL_original = train_kappa
test_kappa_BL_original = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

# for low imbalance data

# seperating target and features
X = df_airline_65.drop(columns=['satisfaction'],axis=1)
y = df_airline_65['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='satisfied')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='satisfied')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_low = train_acc
test_acc_BL_low = test_acc
train_f1_BL_low = train_f1
test_f1_BL_low = test_f1
train_kappa_BL_low = train_kappa
test_kappa_BL_low = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

# for medium imbalance data

# seperating target and features
X = df_airline_75.drop(columns=['satisfaction'],axis=1)
y = df_airline_75['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='satisfied')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='satisfied')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_med = train_acc
test_acc_BL_med = test_acc
train_f1_BL_med = train_f1
test_f1_BL_med = test_f1
train_kappa_BL_med = train_kappa
test_kappa_BL_med = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

# for high imbalance data

# seperating target and features
X = df_airline_90.drop(columns=['satisfaction'],axis=1)
y = df_airline_90['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
train_f1 = []
test_f1 = []
train_kappa = []
test_kappa = []
train_acc = []
test_acc = []
for train_index, test_index in skf.split(X, y):
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  x_train_fold_scale = pd.DataFrame(scale.fit_transform(x_train_fold), columns=x_train_fold.columns)
  x_test_fold_scale = pd.DataFrame(scale.transform(x_test_fold), columns=x_test_fold.columns)
  BL_Model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
  BL_Model.fit(x_train_fold_scale, y_train_fold)
  train_f1_cv = metrics.f1_score(y_train_fold, BL_Model.predict(x_train_fold_scale), pos_label='satisfied')
  test_f1_cv = metrics.f1_score(y_test_fold, BL_Model.predict(x_test_fold_scale), pos_label='satisfied')
  train_kappa_cv = metrics.cohen_kappa_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_kappa_cv = metrics.cohen_kappa_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  train_acc_cv = metrics.accuracy_score(y_train_fold, BL_Model.predict(x_train_fold_scale))#, pos_label='Yes')
  test_acc_cv = metrics.accuracy_score(y_test_fold, BL_Model.predict(x_test_fold_scale))#, pos_label='Yes')
  print('Iteration number:', iteration_no)
  train_f1 = np.append(train_f1, train_f1_cv)
  test_f1 = np.append(test_f1, test_f1_cv)
  train_kappa = np.append(train_kappa, train_kappa_cv)
  test_kappa = np.append(test_kappa, test_kappa_cv)
  train_acc = np.append(train_acc, train_acc_cv)
  test_acc = np.append(test_acc, test_acc_cv)
  iteration_no += 1
print('\nTrain F1 score after 10 fold CV:', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Test F1 score after 10 fold CV:', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
print('Train kappa after 10 fold CV:', round(train_kappa.mean(),5),'+/-', round(train_kappa.std(),5))
print('Test Kappa after 10 fold CV:', round(test_kappa.mean(),5),'+/-', round(test_kappa.std(),5))
print('Train Accuracy after 10 fold CV:', round(train_acc.mean(),5),'+/-', round(train_acc.std(),5))
print('Test Accuracy after 10 fold CV:', round(test_acc.mean(),5),'+/-', round(test_acc.std(),5))
print('')

# saving performance metrics for comparison
train_acc_BL_hig = train_acc
test_acc_BL_hig = test_acc
train_f1_BL_hig = train_f1
test_f1_BL_hig = test_f1
train_kappa_BL_hig = train_kappa
test_kappa_BL_hig = test_kappa

# visualising the performace metrics
visualise = {'Train F1 Score': train_f1, 'Test F1 Score': test_f1,
             'Train Kappa': train_kappa, 'Test Kappa': test_kappa,
             'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
visualise = pd.DataFrame(visualise)
visualise.boxplot()
plt.ylabel('Performace measure')
plt.xticks(rotation=45)
plt.show()

"""**K-means clustering for original data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable, 1 for satisfied and 0 for other
df_airline['satisfaction'] = np.where(df_airline['satisfaction'] == 'neutral or dissatisfied', 0, df_airline['satisfaction'])
df_airline['satisfaction'] = np.where(df_airline['satisfaction'] == 'satisfied', 1, df_airline['satisfaction'])

# checking optimal value of k using elbow method
df_airline_scale = pd.DataFrame(scale_minmax.fit_transform(df_airline), columns=df_airline.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(2,8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_airline_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_airline_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_airline_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_airline['satisfaction'] = np.where(df_airline['satisfaction'] == 0, 'neutral or dissatisfied', df_airline['satisfaction'])
df_airline['satisfaction'] = np.where(df_airline['satisfaction'] == 1, 'satisfied', df_airline['satisfaction'])

# for original data

# seperating target and features
X = df_airline.drop(columns=['satisfaction'],axis=1)
y = df_airline['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_org = train_f1
test_f1_cluster0_org = test_f1

# for original data

# seperating target and features
X = df_airline.drop(columns=['satisfaction'],axis=1)
y = df_airline['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 1]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_org = train_f1
test_f1_cluster1_org = test_f1

# for original data

# seperating target and features
X = df_airline.drop(columns=['satisfaction'],axis=1)
y = df_airline['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 2
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 2]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster2_org = train_f1
test_f1_cluster2_org = test_f1

"""**K-means clustering for low imbalance data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable, 1 for satisfied and 0 for other
df_airline_65['satisfaction'] = np.where(df_airline_65['satisfaction'] == 'neutral or dissatisfied', 0, df_airline_65['satisfaction'])
df_airline_65['satisfaction'] = np.where(df_airline_65['satisfaction'] == 'satisfied', 1, df_airline_65['satisfaction'])

# checking optimal value of k using elbow method
df_airline_scale = pd.DataFrame(scale_minmax.fit_transform(df_airline_65), columns=df_airline_65.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(2,8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_airline_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_airline_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_airline_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_airline_65['satisfaction'] = np.where(df_airline_65['satisfaction'] == 0, 'neutral or dissatisfied', df_airline_65['satisfaction'])
df_airline_65['satisfaction'] = np.where(df_airline_65['satisfaction'] == 1, 'satisfied', df_airline_65['satisfaction'])

# for low imbalance data

# seperating target and features
X = df_airline_65.drop(columns=['satisfaction'],axis=1)
y = df_airline_65['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_low = train_f1
test_f1_cluster0_low = test_f1

# for low imbalance data

# seperating target and features
X = df_airline_65.drop(columns=['satisfaction'],axis=1)
y = df_airline_65['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 1]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_low = train_f1
test_f1_cluster1_low = test_f1

# for low imbalance data

# seperating target and features
X = df_airline_65.drop(columns=['satisfaction'],axis=1)
y = df_airline_65['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 2
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 2]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster2_low = train_f1
test_f1_cluster2_low = test_f1

"""**K-means clustering for medium imbalance data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable, 1 for satisfied and 0 for other
df_airline_75['satisfaction'] = np.where(df_airline_75['satisfaction'] == 'neutral or dissatisfied', 0, df_airline_75['satisfaction'])
df_airline_75['satisfaction'] = np.where(df_airline_75['satisfaction'] == 'satisfied', 1, df_airline_75['satisfaction'])

# checking optimal value of k using elbow method
df_airline_scale = pd.DataFrame(scale_minmax.fit_transform(df_airline_75), columns=df_airline_75.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(2,4)#8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_airline_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_airline_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_airline_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_airline_75['satisfaction'] = np.where(df_airline_75['satisfaction'] == 0, 'neutral or dissatisfied', df_airline_75['satisfaction'])
df_airline_75['satisfaction'] = np.where(df_airline_75['satisfaction'] == 1, 'satisfied', df_airline_75['satisfaction'])

# for medium imbalance data

# seperating target and features
X = df_airline_75.drop(columns=['satisfaction'],axis=1)
y = df_airline_75['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_med = train_f1
test_f1_cluster0_med = test_f1

# for medium imbalance data

# seperating target and features
X = df_airline_75.drop(columns=['satisfaction'],axis=1)
y = df_airline_75['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 1]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_med = train_f1
test_f1_cluster1_med = test_f1

# for medium imbalance data

# seperating target and features
X = df_airline_75.drop(columns=['satisfaction'],axis=1)
y = df_airline_75['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 2
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 2]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster2_med = train_f1
test_f1_cluster2_med = test_f1

"""**K-means clustering for high imbalance data**"""

# configuring min max sacling
scale_minmax = MinMaxScaler()

# Encoding target variable, 1 for satisfied and 0 for other
df_airline_90['satisfaction'] = np.where(df_airline_90['satisfaction'] == 'neutral or dissatisfied', 0, df_airline_90['satisfaction'])
df_airline_90['satisfaction'] = np.where(df_airline_90['satisfaction'] == 'satisfied', 1, df_airline_90['satisfaction'])

# checking optimal value of k using elbow method
df_airline_scale = pd.DataFrame(scale_minmax.fit_transform(df_airline_90), columns=df_airline_90.columns)

# running k-means to find optimum k value

WSS = [] # creating empty list for appending Within sum of squares
K = range(2,8)
for k in K:
    k_means = KMeans(n_clusters=k,random_state=1)
    K_means_model = k_means.fit(df_airline_scale)
    WSS.append(K_means_model.inertia_)
plt.plot(K, WSS, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Sum of Squares')
plt.title('Elbow Method to find optimal k value')
plt.show()

# checking optimal value of k using silhoutte method
K = range(2,8)
No_of_clusters = []
Silhouette_scores = []

for k in K:
  k_means = KMeans(n_clusters=k,random_state=1)
  k_means.fit(df_airline_scale)
  preds = k_means.labels_
  centers = k_means.cluster_centers_
  sil_score = silhouette_score(df_airline_scale,preds)
  No_of_clusters.append(k)
  Silhouette_scores.append(sil_score)
  print(k)

plt.xlabel('k')
plt.ylabel('Silhouette coefficient')
plt.title('Silhouette Method to find optimal k value')
plt.bar(No_of_clusters,Silhouette_scores)
plt.show()

df_airline_90['satisfaction'] = np.where(df_airline_90['satisfaction'] == 0, 'neutral or dissatisfied', df_airline_90['satisfaction'])
df_airline_90['satisfaction'] = np.where(df_airline_90['satisfaction'] == 1, 'satisfied', df_airline_90['satisfaction'])

# for high imbalance data

# seperating target and features
X = df_airline_90.drop(columns=['satisfaction'],axis=1)
y = df_airline_90['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 0
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 0]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster0_hig = train_f1
test_f1_cluster0_hig = test_f1

# for high imbalance data

# seperating target and features
X = df_airline_90.drop(columns=['satisfaction'],axis=1)
y = df_airline_90['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 1
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 1]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster1_hig = train_f1
test_f1_cluster1_hig = test_f1

# for high imbalance data

# seperating target and features
X = df_airline_90.drop(columns=['satisfaction'],axis=1)
y = df_airline_90['satisfaction']

# performing 10 fold stratified cross validation
iteration_no = 1
minority_class_samples_save = []
centers_save = []
train_f1 = []
test_f1 = []
Silhouette_scores = []
for train_index, test_index in skf.split(X, y):
  print('**********Iteration Number {}**********'.format(iteration_no))
  x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  df_train = x_train_fold.copy()
  test_data = x_test_fold.copy()
  test_data['satisfaction'] = y_test_fold 
  df_train['satisfaction'] = y_train_fold
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'neutral or dissatisfied', 0, df_train['satisfaction'])
  df_train['satisfaction'] = np.where(df_train['satisfaction'] == 'satisfied', 1, df_train['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'neutral or dissatisfied', 0, test_data['satisfaction'])
  test_data['satisfaction'] = np.where(test_data['satisfaction'] == 'satisfied', 1, test_data['satisfaction'])

  df_train_scale = pd.DataFrame(scale_minmax.fit_transform(df_train), columns = df_airline.columns) 
  km_3 = KMeans(n_clusters=3,random_state=1)    # k=3 clusters using elbow and silhouette method
  km_3.fit(df_train_scale)
  centers = km_3.cluster_centers_
  centers_save = np.append(centers_save, centers)
  Cluster_3 = km_3.labels_
  sil_score = silhouette_score(df_train_scale,Cluster_3)
  print('Silhouette score:', round(sil_score,5))
  Silhouette_scores.append(sil_score)
  df_airline_cluster = df_train_scale.copy()
  df_airline_cluster['Cluster'] = Cluster_3

  # for cluster label 2
  df_cluster0 = df_airline_cluster[df_airline_cluster['Cluster'] == 2]
  minority_class_samples = df_cluster0.satisfaction.value_counts().min()
  minority_class_samples_save = np.append(minority_class_samples_save, minority_class_samples)
  print('Minority class samples:',minority_class_samples)

  if minority_class_samples != 0:
    features = df_cluster0.drop(columns=['satisfaction'], axis=1)
    labels = df_cluster0['satisfaction']
    rf_model = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=4, min_samples_leaf=1000, min_samples_split=2, random_state=1)
    rf_model.fit(features, labels)
    #print('Train Data')
    train_f1_cv = metrics.f1_score(labels, rf_model.predict(features), pos_label=1)
    print('Train F1 {:.5f}'.format(train_f1_cv))
    train_f1 = np.append(train_f1, train_f1_cv)
    #print(metrics.classification_report(labels, rf_model.predict(features)))
  else:
    print('There are no samples of both classes, Hence classifier is not trained')

  test_data_scale = pd.DataFrame(scale_minmax.transform(test_data), columns = df_airline.columns)
  cluster_test_labels = km_3.fit_predict(test_data_scale)
  test_data_scale['Cluster'] = cluster_test_labels

  if minority_class_samples != 0:
    X_test = test_data_scale.drop(columns=['satisfaction'], axis=1)
    y_test = test_data_scale['satisfaction']
    #print('\nTest data')
    test_f1_cv = metrics.f1_score(y_test, rf_model.predict(X_test), pos_label=1)
    print('Test F1 {:.5f}'.format(test_f1_cv))
    test_f1 = np.append(test_f1, test_f1_cv)
    #print(metrics.classification_report(y_test, rf_model.predict(X_test)))
  else:
    print('There are no samples of both classes')
  iteration_no += 1
print('\nMinority class samples during each iterations', minority_class_samples_save)
print('Silhouette scores', round(np.mean(Silhouette_scores),5),'+/-', round(np.std(Silhouette_scores),5))
print('Overall train F1 score', round(train_f1.mean(),5),'+/-', round(train_f1.std(),5))
print('Overall test F1 score', round(test_f1.mean(),5),'+/-', round(test_f1.std(),5))
train_f1_cluster2_hig = train_f1
test_f1_cluster2_hig = test_f1

"""**Comparison using permutation test**"""

from numpy.random.mtrand import permutation
def get_pvalue(iterations,Results1,Results2,diff):
  concat = np.concatenate((Results1,Results2))
  count = 0
  for i in range(0,iterations):
    permutation = np.random.permutation(concat)
    p_current = permutation[:len(Results1)]
    p_new = permutation[len(Results1):]
    mean_permutation_current = p_current.mean()
    mean_permutation_new = p_new.mean()
    t_permutation = mean_permutation_new - mean_permutation_current

    if(t_permutation > diff):
      count += 1
  p_value = count / iterations
  if p_value > 0.05:
    print('Since p value {} is greater than 0.05 we failed to reject null hypothesis'.format(round(p_value,4)))
    #print('Results 1 are not better than Results 2')
  else:
    print('Since p value {} is less than 0.05 we will reject null hypothesis'.format(round(p_value,4)))
    #print('Results 1 are better than Results 2')
  return #p_value

# comaprison of baseline model for different imbalances
comparison = pd.concat([pd.Series(test_f1_BL_original), pd.Series(test_f1_BL_low), pd.Series(test_f1_BL_med), pd.Series(test_f1_BL_hig)], axis=1)
comparison.columns = ['Original Data', 'Low Imbalance', 'Medium Imbalance', 'high Imbalance']
comparison.boxplot()
plt.ylabel('F1-score')
plt.xticks(rotation=45)
plt.show()

# comaprison of custom model for different imbalances
comparison1 = pd.concat([pd.Series(test_f1_cluster0_org), pd.Series(test_f1_cluster1_org), pd.Series(test_f1_cluster2_org), pd.Series(test_f1_cluster0_low), pd.Series(test_f1_cluster1_low), pd.Series(test_f1_cluster2_low), pd.Series(test_f1_cluster0_med), pd.Series(test_f1_cluster1_med), pd.Series(test_f1_cluster2_med), pd.Series(test_f1_cluster0_hig), pd.Series(test_f1_cluster1_hig), pd.Series(test_f1_cluster2_hig)], axis=1)
comparison1.columns = ['Org. Cluster 0 ', 'Org. Cluster 1','Org. Cluster 2', 'Low. Cluster 0 ', 'Low. Cluster 1', 'Low. Cluster 2', 'Med. Cluster 0', 'Med. Cluster 1', 'Med. Cluster 2', 'Hig. Cluster 0', 'Hig. Cluster 1', 'Hig. Cluster 2']
comparison1.boxplot()
plt.ylabel('F1-score')
plt.xticks(rotation=90)
plt.show()

comparison

comparison1

"""**Comparison between base and custom models**"""

# original data comparison base line Vs custo model
diff = test_f1_BL_original.mean() - test_f1_cluster0_org.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_original, test_f1_cluster0_org, diff)

diff = test_f1_BL_original.mean() - test_f1_cluster1_org.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_original, test_f1_cluster1_org, diff)

diff = test_f1_BL_original.mean() - test_f1_cluster2_org.mean()    #for cluster 2
get_pvalue(10000, test_f1_BL_original, test_f1_cluster2_org, diff)

# low data imbalance comparison base line Vs custo model
diff = test_f1_BL_low.mean() - test_f1_cluster0_low.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_low, test_f1_cluster0_low, diff)

diff = test_f1_BL_low.mean() - test_f1_cluster1_low.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_low, test_f1_cluster1_low, diff)

diff = test_f1_BL_low.mean() - test_f1_cluster2_low.mean()    #for cluster 2
get_pvalue(10000, test_f1_BL_low, test_f1_cluster2_low, diff)

# Medium data imbalance comparison base line Vs custo model
diff = test_f1_BL_med.mean() - test_f1_cluster0_med.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_med, test_f1_cluster0_med, diff)

diff = test_f1_BL_med.mean() - test_f1_cluster1_med.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_med, test_f1_cluster1_med, diff)

diff = test_f1_BL_med.mean() - test_f1_cluster2_med.mean()    #for cluster 2
get_pvalue(10000, test_f1_BL_med, test_f1_cluster2_med, diff)

# High data imbalance comparison base line Vs custo model
diff = test_f1_BL_hig.mean() - test_f1_cluster0_hig.mean()    #for cluster 0
get_pvalue(10000, test_f1_BL_hig, test_f1_cluster0_hig, diff)

diff = test_f1_BL_hig.mean() - test_f1_cluster1_hig.mean()    #for cluster 1
get_pvalue(10000, test_f1_BL_hig, test_f1_cluster1_hig, diff)

diff = test_f1_BL_hig.mean() - test_f1_cluster2_hig.mean()    #for cluster 2
get_pvalue(10000, test_f1_BL_hig, test_f1_cluster2_hig, diff)